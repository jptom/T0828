{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqiyOJv8c7rn",
        "outputId": "e5b0c18e-c182-4a1d-8303-15c5dd2d1642"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUEZ8FsCdRZ2",
        "outputId": "6087aea7-15c0-479c-a228-d08d9d33466f"
      },
      "source": [
        "HOME_PATH = \"/content\"\r\n",
        "%cd \"$HOME_PATH\"\r\n",
        "!git clone https://github.com/open-mmlab/mmdetection.git\r\n",
        "%cd mmdetection\r\n",
        "!pip install -r requirements/build.txt\r\n",
        "!pip install \"git+https://github.com/open-mmlab/cocoapi.git#subdirectory=pycocotools\"\r\n",
        "!pip install -v -e .  # or \"python setup.py develop\"\r\n",
        "!pip install mmcv-full"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'mmdetection'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 14900 (delta 4), reused 0 (delta 0), pack-reused 14878\u001b[K\n",
            "Receiving objects: 100% (14900/14900), 14.34 MiB | 33.08 MiB/s, done.\n",
            "Resolving deltas: 100% (10167/10167), done.\n",
            "/content/mmdetection\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from -r requirements/build.txt (line 2)) (0.29.21)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements/build.txt (line 3)) (1.18.5)\n",
            "Collecting git+https://github.com/open-mmlab/cocoapi.git#subdirectory=pycocotools\n",
            "  Cloning https://github.com/open-mmlab/cocoapi.git to /tmp/pip-req-build-fjzlwz04\n",
            "  Running command git clone -q https://github.com/open-mmlab/cocoapi.git /tmp/pip-req-build-fjzlwz04\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from mmpycocotools==12.0.3) (50.3.2)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from mmpycocotools==12.0.3) (0.29.21)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from mmpycocotools==12.0.3) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->mmpycocotools==12.0.3) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->mmpycocotools==12.0.3) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->mmpycocotools==12.0.3) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->mmpycocotools==12.0.3) (1.18.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->mmpycocotools==12.0.3) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->mmpycocotools==12.0.3) (1.15.0)\n",
            "Building wheels for collected packages: mmpycocotools\n",
            "  Building wheel for mmpycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mmpycocotools: filename=mmpycocotools-12.0.3-cp36-cp36m-linux_x86_64.whl size=266784 sha256=60547a88db2be380a6c0e1102295b688c8562b965a1a5f4864f7175f2382d10e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-akmlnxnc/wheels/cd/f6/de/018ccc2d175046c612e93b42a169cd1ab7563d61581cfba8df\n",
            "Successfully built mmpycocotools\n",
            "Installing collected packages: mmpycocotools\n",
            "Successfully installed mmpycocotools-12.0.3\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-eccut8rj\n",
            "Created temporary directory: /tmp/pip-req-tracker-15ei9kh4\n",
            "Created requirements tracker '/tmp/pip-req-tracker-15ei9kh4'\n",
            "Created temporary directory: /tmp/pip-install-_oi7hjkt\n",
            "Obtaining file:///content/mmdetection\n",
            "  Added file:///content/mmdetection to build tracker '/tmp/pip-req-tracker-15ei9kh4'\n",
            "    Running setup.py (path:/content/mmdetection/setup.py) egg_info for package from file:///content/mmdetection\n",
            "    Running command python setup.py egg_info\n",
            "    running egg_info\n",
            "    creating mmdet.egg-info\n",
            "    writing mmdet.egg-info/PKG-INFO\n",
            "    writing dependency_links to mmdet.egg-info/dependency_links.txt\n",
            "    writing requirements to mmdet.egg-info/requires.txt\n",
            "    writing top-level names to mmdet.egg-info/top_level.txt\n",
            "    writing manifest file 'mmdet.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'mmdet.egg-info/SOURCES.txt'\n",
            "  Source in /content/mmdetection has version 2.7.0, which satisfies requirement mmdet==2.7.0 from file:///content/mmdetection\n",
            "  Removed mmdet==2.7.0 from file:///content/mmdetection from build tracker '/tmp/pip-req-tracker-15ei9kh4'\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from mmdet==2.7.0) (3.2.2)\n",
            "Requirement already satisfied: mmpycocotools in /usr/local/lib/python3.6/dist-packages (from mmdet==2.7.0) (12.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mmdet==2.7.0) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from mmdet==2.7.0) (1.15.0)\n",
            "1 location(s) to search for versions of terminaltables:\n",
            "* https://pypi.org/simple/terminaltables/\n",
            "Getting page https://pypi.org/simple/terminaltables/\n",
            "Found index url https://pypi.org/simple\n",
            "Looking up \"https://pypi.org/simple/terminaltables/\" in the cache\n",
            "Request header has \"max_age\" as 0, cache bypassed\n",
            "Starting new HTTPS connection (1): pypi.org:443\n",
            "https://pypi.org:443 \"GET /simple/terminaltables/ HTTP/1.1\" 200 1186\n",
            "Updating cache with response from \"https://pypi.org/simple/terminaltables/\"\n",
            "Caching due to etag\n",
            "Analyzing links from page https://pypi.org/simple/terminaltables/\n",
            "  Found link https://files.pythonhosted.org/packages/ec/82/6390ba7f110622d27b02451aaa294dc4b3133b7661e464db9a116e977324/terminaltables-1.0.0.tar.gz#sha256=4c909a5ee4a3d028b2c977d996f8b8cd9724ce8e4d9d834d65e78a98f7965b54 (from https://pypi.org/simple/terminaltables/), version: 1.0.0\n",
            "  Found link https://files.pythonhosted.org/packages/97/65/858bc3ea6cc60edc959ce427a94227932b5d9a95b0bce82f16071419885c/terminaltables-1.0.1.tar.gz#sha256=5548ac567d38d6ac88a5e0fec2d95f646249f37e1ef8fd2d17f8fcaefc6cf592 (from https://pypi.org/simple/terminaltables/), version: 1.0.1\n",
            "  Found link https://files.pythonhosted.org/packages/82/42/3f1140f6e538582fd514c765244662cca60885048cf610e7d00eaee8aeb1/terminaltables-1.0.2.tar.gz#sha256=cf97dd019af975cc64aa69aca435a43b0cffabb88df6f337c6b48de600c19f8e (from https://pypi.org/simple/terminaltables/), version: 1.0.2\n",
            "  Found link https://files.pythonhosted.org/packages/80/07/5663569dfd8fa4e4fa3cb645b70f4972e3d79d056b71da12df174668c145/terminaltables-1.1.0.tar.gz#sha256=94a15e1a295265d130de67e9c2efef9e1cad1e64dd6ae0b80882076581605f8c (from https://pypi.org/simple/terminaltables/), version: 1.1.0\n",
            "  Found link https://files.pythonhosted.org/packages/0c/4a/9b80642ac2463908fe77c9dbe138c56902fbf5a5a95d07203c131ec9ba90/terminaltables-1.1.1.tar.gz#sha256=b02c516d6d521ce0fe6e2a2753268e86547bbccab6bfa7e269a0f51766283fab (from https://pypi.org/simple/terminaltables/), version: 1.1.1\n",
            "  Found link https://files.pythonhosted.org/packages/a8/65/f9c6bcfb1f81acdfcd1f8d633c6752cfdcc04b5fade7638a2a8dc7a720de/terminaltables-1.2.0.tar.gz#sha256=fff4aa62f296038d1526a91856f0b3de1e3bce31cfd1c5148cc3f795c1d396bf (from https://pypi.org/simple/terminaltables/), version: 1.2.0\n",
            "  Found link https://files.pythonhosted.org/packages/3d/17/14aa6521b337be46c51dd7b31e7e617801e9f8db7f48583c767c02e0e72a/terminaltables-1.2.1.tar.gz#sha256=cf5f0fb6c6c3070d7af73537ded030858c122f253c87e7221f9a6da3782ce787 (from https://pypi.org/simple/terminaltables/), version: 1.2.1\n",
            "  Found link https://files.pythonhosted.org/packages/d0/8e/9403573ff8aebc09ee0aacd57885050f74bd9f48a85c0735d33cacfa2469/terminaltables-2.0.0.tar.gz#sha256=2e0a6688071f2a881f8fa4455a362457dcd2317e374609f1a09baffa998e7492 (from https://pypi.org/simple/terminaltables/), version: 2.0.0\n",
            "  Found link https://files.pythonhosted.org/packages/10/da/9bbb21c1c2f9be4df2056b00b569689b9ece538ef39bf8db34be25f9e850/terminaltables-2.1.0.tar.gz#sha256=33b60f027964214f4ff5821f43958d03add81784f7c183d86a7ee8f010350cf5 (from https://pypi.org/simple/terminaltables/), version: 2.1.0\n",
            "  Found link https://files.pythonhosted.org/packages/58/c9/f0c174c4e828365df3593c66ac32474cd994a8ec36fe19a798261c96c3bc/terminaltables-3.0.0.tar.gz#sha256=bd2504031f09f942a8f221266adc61aee04a0368d5de0dacb7a53e508af6a518 (from https://pypi.org/simple/terminaltables/), version: 3.0.0\n",
            "  Found link https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz#sha256=f3eb0eb92e3833972ac36796293ca0906e998dc3be91fbe1f8615b331b853b81 (from https://pypi.org/simple/terminaltables/), version: 3.1.0\n",
            "Given no hashes to check 11 links for project 'terminaltables': discarding no candidates\n",
            "Using version 3.1.0 (newest of versions: 1.0.0, 1.0.1, 1.0.2, 1.1.0, 1.1.1, 1.2.0, 1.2.1, 2.0.0, 2.1.0, 3.0.0, 3.1.0)\n",
            "Collecting terminaltables\n",
            "  Created temporary directory: /tmp/pip-unpack-k_qygojh\n",
            "  Looking up \"https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\" in the cache\n",
            "  No cache entry available\n",
            "  Starting new HTTPS connection (1): files.pythonhosted.org:443\n",
            "  https://files.pythonhosted.org:443 \"GET /packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz HTTP/1.1\" 200 12478\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "  Ignoring unknown cache-control directive: immutable\n",
            "  Updating cache with response from \"https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\"\n",
            "  Caching due to etag\n",
            "  Added terminaltables from https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz#sha256=f3eb0eb92e3833972ac36796293ca0906e998dc3be91fbe1f8615b331b853b81 (from mmdet==2.7.0) to build tracker '/tmp/pip-req-tracker-15ei9kh4'\n",
            "    Running setup.py (path:/tmp/pip-install-_oi7hjkt/terminaltables/setup.py) egg_info for package terminaltables\n",
            "    Running command python setup.py egg_info\n",
            "    running egg_info\n",
            "    creating /tmp/pip-install-_oi7hjkt/terminaltables/pip-egg-info/terminaltables.egg-info\n",
            "    writing /tmp/pip-install-_oi7hjkt/terminaltables/pip-egg-info/terminaltables.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-install-_oi7hjkt/terminaltables/pip-egg-info/terminaltables.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-install-_oi7hjkt/terminaltables/pip-egg-info/terminaltables.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-install-_oi7hjkt/terminaltables/pip-egg-info/terminaltables.egg-info/SOURCES.txt'\n",
            "    reading manifest file '/tmp/pip-install-_oi7hjkt/terminaltables/pip-egg-info/terminaltables.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-install-_oi7hjkt/terminaltables/pip-egg-info/terminaltables.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-install-_oi7hjkt/terminaltables has version 3.1.0, which satisfies requirement terminaltables from https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz#sha256=f3eb0eb92e3833972ac36796293ca0906e998dc3be91fbe1f8615b331b853b81 (from mmdet==2.7.0)\n",
            "  Removed terminaltables from https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz#sha256=f3eb0eb92e3833972ac36796293ca0906e998dc3be91fbe1f8615b331b853b81 (from mmdet==2.7.0) from build tracker '/tmp/pip-req-tracker-15ei9kh4'\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mmdet==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mmdet==2.7.0) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mmdet==2.7.0) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mmdet==2.7.0) (2.8.1)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from mmpycocotools->mmdet==2.7.0) (0.29.21)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from mmpycocotools->mmdet==2.7.0) (50.3.2)\n",
            "Building wheels for collected packages: terminaltables\n",
            "  Created temporary directory: /tmp/pip-wheel-7ic0lken\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-7ic0lken\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-_oi7hjkt/terminaltables/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-_oi7hjkt/terminaltables/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-7ic0lken --python-tag cp36\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/terminaltables\n",
            "  copying terminaltables/base_table.py -> build/lib/terminaltables\n",
            "  copying terminaltables/terminal_io.py -> build/lib/terminaltables\n",
            "  copying terminaltables/__init__.py -> build/lib/terminaltables\n",
            "  copying terminaltables/ascii_table.py -> build/lib/terminaltables\n",
            "  copying terminaltables/github_table.py -> build/lib/terminaltables\n",
            "  copying terminaltables/width_and_alignment.py -> build/lib/terminaltables\n",
            "  copying terminaltables/build.py -> build/lib/terminaltables\n",
            "  copying terminaltables/other_tables.py -> build/lib/terminaltables\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/terminaltables\n",
            "  copying build/lib/terminaltables/base_table.py -> build/bdist.linux-x86_64/wheel/terminaltables\n",
            "  copying build/lib/terminaltables/terminal_io.py -> build/bdist.linux-x86_64/wheel/terminaltables\n",
            "  copying build/lib/terminaltables/__init__.py -> build/bdist.linux-x86_64/wheel/terminaltables\n",
            "  copying build/lib/terminaltables/ascii_table.py -> build/bdist.linux-x86_64/wheel/terminaltables\n",
            "  copying build/lib/terminaltables/github_table.py -> build/bdist.linux-x86_64/wheel/terminaltables\n",
            "  copying build/lib/terminaltables/width_and_alignment.py -> build/bdist.linux-x86_64/wheel/terminaltables\n",
            "  copying build/lib/terminaltables/build.py -> build/bdist.linux-x86_64/wheel/terminaltables\n",
            "  copying build/lib/terminaltables/other_tables.py -> build/bdist.linux-x86_64/wheel/terminaltables\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  writing terminaltables.egg-info/PKG-INFO\n",
            "  writing dependency_links to terminaltables.egg-info/dependency_links.txt\n",
            "  writing top-level names to terminaltables.egg-info/top_level.txt\n",
            "  reading manifest file 'terminaltables.egg-info/SOURCES.txt'\n",
            "  writing manifest file 'terminaltables.egg-info/SOURCES.txt'\n",
            "  Copying terminaltables.egg-info to build/bdist.linux-x86_64/wheel/terminaltables-3.1.0-py3.6.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/terminaltables-3.1.0.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-7ic0lken/terminaltables-3.1.0-cp36-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'terminaltables/__init__.py'\n",
            "  adding 'terminaltables/ascii_table.py'\n",
            "  adding 'terminaltables/base_table.py'\n",
            "  adding 'terminaltables/build.py'\n",
            "  adding 'terminaltables/github_table.py'\n",
            "  adding 'terminaltables/other_tables.py'\n",
            "  adding 'terminaltables/terminal_io.py'\n",
            "  adding 'terminaltables/width_and_alignment.py'\n",
            "  adding 'terminaltables-3.1.0.dist-info/METADATA'\n",
            "  adding 'terminaltables-3.1.0.dist-info/WHEEL'\n",
            "  adding 'terminaltables-3.1.0.dist-info/top_level.txt'\n",
            "  adding 'terminaltables-3.1.0.dist-info/zip-safe'\n",
            "  adding 'terminaltables-3.1.0.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15357 sha256=8fb140249d73badffe604a36397dbb326e2d0ef6964a03075bdef86950ec41c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "  Removing source in /tmp/pip-install-_oi7hjkt/terminaltables\n",
            "Successfully built terminaltables\n",
            "Installing collected packages: terminaltables, mmdet\n",
            "\n",
            "  Running setup.py develop for mmdet\n",
            "    Running command /usr/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/content/mmdetection/setup.py'\"'\"'; __file__='\"'\"'/content/mmdetection/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps\n",
            "    running develop\n",
            "    running egg_info\n",
            "    writing mmdet.egg-info/PKG-INFO\n",
            "    writing dependency_links to mmdet.egg-info/dependency_links.txt\n",
            "    writing requirements to mmdet.egg-info/requires.txt\n",
            "    writing top-level names to mmdet.egg-info/top_level.txt\n",
            "    writing manifest file 'mmdet.egg-info/SOURCES.txt'\n",
            "    running build_ext\n",
            "    Creating /usr/local/lib/python3.6/dist-packages/mmdet.egg-link (link to .)\n",
            "    Adding mmdet 2.7.0 to easy-install.pth file\n",
            "\n",
            "    Installed /content/mmdetection\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:339: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "Successfully installed mmdet terminaltables-3.1.0\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-15ei9kh4'\n",
            "Collecting mmcv-full\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/ca/fa9f0aedeb6c2f701996a71ef03e365c2870b1bb0ea63f07459b1ef9128e/mmcv-full-1.2.1.tar.gz (251kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 8.7MB/s \n",
            "\u001b[?25hCollecting addict\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/00/b08f23b7d7e1e14ce01419a467b583edbb93c6cdb8654e54a9cc579cd61f/addict-2.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mmcv-full) (1.18.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from mmcv-full) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from mmcv-full) (3.13)\n",
            "Collecting yapf\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/5d/d84677fe852bc5e091739acda444a9b6700ffc6b11a21b00dd244c8caef0/yapf-0.30.0-py2.py3-none-any.whl (190kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 15.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mmcv-full\n",
            "  Building wheel for mmcv-full (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mmcv-full: filename=mmcv_full-1.2.1-cp36-cp36m-linux_x86_64.whl size=20693435 sha256=9a716be731e06cb6291f52ec9c3e8bf533305b2b4d76a4b362500867799f3647\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/0b/39/0cec97174407144208eed51efda0169de0860c0b8460ed0855\n",
            "Successfully built mmcv-full\n",
            "Installing collected packages: addict, yapf, mmcv-full\n",
            "Successfully installed addict-2.4.0 mmcv-full-1.2.1 yapf-0.30.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2OsWT1mda-i",
        "outputId": "f82137ec-29b3-47be-f887-edfe59c91f21"
      },
      "source": [
        "from mmdet.apis import set_random_seed\r\n",
        "from mmcv import Config\r\n",
        "cfg = Config.fromfile('/content/gdrive/MyDrive/mmdetection/configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py')\r\n",
        "\r\n",
        "cfg.dataset_type = 'VocTiny'\r\n",
        "cfg.data_root = '/content/gdrive/MyDrive/T0828/HW3/dataset'\r\n",
        "\r\n",
        "cfg.data.test.type = 'VocTiny'\r\n",
        "cfg.data.test.data_root = '/content/gdrive/MyDrive/T0828/HW3/dataset'\r\n",
        "cfg.data.test.ann_file = 'test.json'\r\n",
        "cfg.data.test.img_prefix = '/content/gdrive/MyDrive/T0828/HW3/dataset/test_images'\r\n",
        "\r\n",
        "cfg.data.train.type = 'VocTiny'\r\n",
        "cfg.data.train.data_root = '/content/gdrive/MyDrive/T0828/HW3/dataset'\r\n",
        "cfg.data.train.ann_file = 'train.json'\r\n",
        "cfg.data.train.img_prefix = '/content/gdrive/MyDrive/T0828/HW3/dataset/train_images'\r\n",
        "\r\n",
        "cfg.data.val.type = 'VocTiny'\r\n",
        "cfg.data.val.data_root = '/content/gdrive/MyDrive/T0828/HW3/dataset'\r\n",
        "cfg.data.val.ann_file = 'train.json'\r\n",
        "cfg.data.val.img_prefix = '/content/gdrive/MyDrive/T0828/HW3/dataset/train_images'\r\n",
        "\r\n",
        "cfg.model.roi_head.bbox_head.num_classes = 20\r\n",
        "cfg.model.roi_head.mask_head.num_classes = 20\r\n",
        "#cfg.load_from = '/content/gdrive/MyDrive/T0828/HW3/log/epoch_20.pth'\r\n",
        "cfg.work_dir = '/content/gdrive/MyDrive/T0828/HW3/log'\r\n",
        "\r\n",
        "# Change the evaluation metric since we use customized dataset.\r\n",
        "cfg.evaluation.metric = 'mAP'\r\n",
        "# We can set the evaluation interval to reduce the evaluation times\r\n",
        "cfg.evaluation.interval = 12\r\n",
        "# We can set the checkpoint saving interval to reduce the storage cost\r\n",
        "cfg.checkpoint_config.interval = 12\r\n",
        "\r\n",
        "cfg.total_epochs = 24####################################\r\n",
        "# Set seed thus the results are more reproducible\r\n",
        "cfg.seed = 12\r\n",
        "set_random_seed(12, deterministic=False)\r\n",
        "cfg.gpu_ids = range(1)###############################\r\n",
        "cfg.data['samples_per_gpu'] = 4\r\n",
        "cfg.data['workers_per_gpu'] = 4\r\n",
        "print(f'Config:\\n{cfg.pretty_text}')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config:\n",
            "model = dict(\n",
            "    type='MaskRCNN',\n",
            "    pretrained='torchvision://resnet50',\n",
            "    backbone=dict(\n",
            "        type='ResNet',\n",
            "        depth=50,\n",
            "        num_stages=4,\n",
            "        out_indices=(0, 1, 2, 3),\n",
            "        frozen_stages=1,\n",
            "        norm_cfg=dict(type='BN', requires_grad=True),\n",
            "        norm_eval=True,\n",
            "        style='pytorch'),\n",
            "    neck=dict(\n",
            "        type='FPN',\n",
            "        in_channels=[256, 512, 1024, 2048],\n",
            "        out_channels=256,\n",
            "        num_outs=5),\n",
            "    rpn_head=dict(\n",
            "        type='RPNHead',\n",
            "        in_channels=256,\n",
            "        feat_channels=256,\n",
            "        anchor_generator=dict(\n",
            "            type='AnchorGenerator',\n",
            "            scales=[8],\n",
            "            ratios=[0.5, 1.0, 2.0],\n",
            "            strides=[4, 8, 16, 32, 64]),\n",
            "        bbox_coder=dict(\n",
            "            type='DeltaXYWHBBoxCoder',\n",
            "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
            "        loss_cls=dict(\n",
            "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
            "        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
            "    roi_head=dict(\n",
            "        type='StandardRoIHead',\n",
            "        bbox_roi_extractor=dict(\n",
            "            type='SingleRoIExtractor',\n",
            "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
            "            out_channels=256,\n",
            "            featmap_strides=[4, 8, 16, 32]),\n",
            "        bbox_head=dict(\n",
            "            type='Shared2FCBBoxHead',\n",
            "            in_channels=256,\n",
            "            fc_out_channels=1024,\n",
            "            roi_feat_size=7,\n",
            "            num_classes=20,\n",
            "            bbox_coder=dict(\n",
            "                type='DeltaXYWHBBoxCoder',\n",
            "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "                target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
            "            reg_class_agnostic=False,\n",
            "            loss_cls=dict(\n",
            "                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n",
            "            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
            "        mask_roi_extractor=dict(\n",
            "            type='SingleRoIExtractor',\n",
            "            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),\n",
            "            out_channels=256,\n",
            "            featmap_strides=[4, 8, 16, 32]),\n",
            "        mask_head=dict(\n",
            "            type='FCNMaskHead',\n",
            "            num_convs=4,\n",
            "            in_channels=256,\n",
            "            conv_out_channels=256,\n",
            "            num_classes=20,\n",
            "            loss_mask=dict(\n",
            "                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))\n",
            "train_cfg = dict(\n",
            "    rpn=dict(\n",
            "        assigner=dict(\n",
            "            type='MaxIoUAssigner',\n",
            "            pos_iou_thr=0.7,\n",
            "            neg_iou_thr=0.3,\n",
            "            min_pos_iou=0.3,\n",
            "            match_low_quality=True,\n",
            "            ignore_iof_thr=-1),\n",
            "        sampler=dict(\n",
            "            type='RandomSampler',\n",
            "            num=256,\n",
            "            pos_fraction=0.5,\n",
            "            neg_pos_ub=-1,\n",
            "            add_gt_as_proposals=False),\n",
            "        allowed_border=-1,\n",
            "        pos_weight=-1,\n",
            "        debug=False),\n",
            "    rpn_proposal=dict(\n",
            "        nms_across_levels=False,\n",
            "        nms_pre=2000,\n",
            "        nms_post=1000,\n",
            "        max_num=1000,\n",
            "        nms_thr=0.7,\n",
            "        min_bbox_size=0),\n",
            "    rcnn=dict(\n",
            "        assigner=dict(\n",
            "            type='MaxIoUAssigner',\n",
            "            pos_iou_thr=0.5,\n",
            "            neg_iou_thr=0.5,\n",
            "            min_pos_iou=0.5,\n",
            "            match_low_quality=True,\n",
            "            ignore_iof_thr=-1),\n",
            "        sampler=dict(\n",
            "            type='RandomSampler',\n",
            "            num=512,\n",
            "            pos_fraction=0.25,\n",
            "            neg_pos_ub=-1,\n",
            "            add_gt_as_proposals=True),\n",
            "        mask_size=28,\n",
            "        pos_weight=-1,\n",
            "        debug=False))\n",
            "test_cfg = dict(\n",
            "    rpn=dict(\n",
            "        nms_across_levels=False,\n",
            "        nms_pre=1000,\n",
            "        nms_post=1000,\n",
            "        max_num=1000,\n",
            "        nms_thr=0.7,\n",
            "        min_bbox_size=0),\n",
            "    rcnn=dict(\n",
            "        score_thr=0.05,\n",
            "        nms=dict(type='nms', iou_threshold=0.5),\n",
            "        max_per_img=100,\n",
            "        mask_thr_binary=0.5))\n",
            "dataset_type = 'VocTiny'\n",
            "data_root = '/content/gdrive/MyDrive/T0828/HW3/dataset'\n",
            "img_norm_cfg = dict(\n",
            "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
            "train_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
            "    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
            "    dict(type='RandomFlip', flip_ratio=0.5),\n",
            "    dict(\n",
            "        type='Normalize',\n",
            "        mean=[123.675, 116.28, 103.53],\n",
            "        std=[58.395, 57.12, 57.375],\n",
            "        to_rgb=True),\n",
            "    dict(type='Pad', size_divisor=32),\n",
            "    dict(type='DefaultFormatBundle'),\n",
            "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(\n",
            "        type='MultiScaleFlipAug',\n",
            "        img_scale=(1333, 800),\n",
            "        flip=False,\n",
            "        transforms=[\n",
            "            dict(type='Resize', keep_ratio=True),\n",
            "            dict(type='RandomFlip'),\n",
            "            dict(\n",
            "                type='Normalize',\n",
            "                mean=[123.675, 116.28, 103.53],\n",
            "                std=[58.395, 57.12, 57.375],\n",
            "                to_rgb=True),\n",
            "            dict(type='Pad', size_divisor=32),\n",
            "            dict(type='ImageToTensor', keys=['img']),\n",
            "            dict(type='Collect', keys=['img'])\n",
            "        ])\n",
            "]\n",
            "data = dict(\n",
            "    samples_per_gpu=4,\n",
            "    workers_per_gpu=4,\n",
            "    train=dict(\n",
            "        type='VocTiny',\n",
            "        ann_file='train.json',\n",
            "        img_prefix='/content/gdrive/MyDrive/T0828/HW3/dataset/train_images',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
            "            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
            "            dict(type='RandomFlip', flip_ratio=0.5),\n",
            "            dict(\n",
            "                type='Normalize',\n",
            "                mean=[123.675, 116.28, 103.53],\n",
            "                std=[58.395, 57.12, 57.375],\n",
            "                to_rgb=True),\n",
            "            dict(type='Pad', size_divisor=32),\n",
            "            dict(type='DefaultFormatBundle'),\n",
            "            dict(\n",
            "                type='Collect',\n",
            "                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
            "        ],\n",
            "        data_root='/content/gdrive/MyDrive/T0828/HW3/dataset'),\n",
            "    val=dict(\n",
            "        type='VocTiny',\n",
            "        ann_file='train.json',\n",
            "        img_prefix='/content/gdrive/MyDrive/T0828/HW3/dataset/train_images',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='MultiScaleFlipAug',\n",
            "                img_scale=(1333, 800),\n",
            "                flip=False,\n",
            "                transforms=[\n",
            "                    dict(type='Resize', keep_ratio=True),\n",
            "                    dict(type='RandomFlip'),\n",
            "                    dict(\n",
            "                        type='Normalize',\n",
            "                        mean=[123.675, 116.28, 103.53],\n",
            "                        std=[58.395, 57.12, 57.375],\n",
            "                        to_rgb=True),\n",
            "                    dict(type='Pad', size_divisor=32),\n",
            "                    dict(type='ImageToTensor', keys=['img']),\n",
            "                    dict(type='Collect', keys=['img'])\n",
            "                ])\n",
            "        ],\n",
            "        data_root='/content/gdrive/MyDrive/T0828/HW3/dataset'),\n",
            "    test=dict(\n",
            "        type='VocTiny',\n",
            "        ann_file='test.json',\n",
            "        img_prefix='/content/gdrive/MyDrive/T0828/HW3/dataset/test_images',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='MultiScaleFlipAug',\n",
            "                img_scale=(1333, 800),\n",
            "                flip=False,\n",
            "                transforms=[\n",
            "                    dict(type='Resize', keep_ratio=True),\n",
            "                    dict(type='RandomFlip'),\n",
            "                    dict(\n",
            "                        type='Normalize',\n",
            "                        mean=[123.675, 116.28, 103.53],\n",
            "                        std=[58.395, 57.12, 57.375],\n",
            "                        to_rgb=True),\n",
            "                    dict(type='Pad', size_divisor=32),\n",
            "                    dict(type='ImageToTensor', keys=['img']),\n",
            "                    dict(type='Collect', keys=['img'])\n",
            "                ])\n",
            "        ],\n",
            "        data_root='/content/gdrive/MyDrive/T0828/HW3/dataset'))\n",
            "evaluation = dict(metric='mAP', interval=12)\n",
            "optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n",
            "optimizer_config = dict(grad_clip=None)\n",
            "lr_config = dict(\n",
            "    policy='step',\n",
            "    warmup='linear',\n",
            "    warmup_iters=500,\n",
            "    warmup_ratio=0.001,\n",
            "    step=[8, 11])\n",
            "total_epochs = 24\n",
            "checkpoint_config = dict(interval=12)\n",
            "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
            "dist_params = dict(backend='nccl')\n",
            "log_level = 'INFO'\n",
            "load_from = None\n",
            "resume_from = None\n",
            "workflow = [('train', 1)]\n",
            "work_dir = '/content/gdrive/MyDrive/T0828/HW3/log'\n",
            "seed = 12\n",
            "gpu_ids = range(0, 1)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcMCcA5Hdf6Y"
      },
      "source": [
        "from mmdet.datasets import build_dataset\r\n",
        "from mmdet.models import build_detector\r\n",
        "from mmdet.apis import train_detector\r\n",
        "from pycocotools.coco import COCO\r\n",
        "from pycocotools.cocoeval import COCOeval\r\n",
        "import mmcv\r\n",
        "import os.path as osp\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from mmdet.datasets.builder import DATASETS\r\n",
        "from mmdet.datasets.custom import CustomDataset\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from itertools import groupby\r\n",
        "from pycocotools import mask as maskutil\r\n",
        "\r\n",
        "def binary_mask_to_rle(binary_mask):\r\n",
        "    rle = {'counts': [], 'size': list(binary_mask.shape)}\r\n",
        "    counts = rle.get('counts')\r\n",
        "    for i, (value, elements) in enumerate(groupby(binary_mask.ravel(order='F'))):\r\n",
        "        if i == 0 and value == 1:\r\n",
        "            counts.append(0)\r\n",
        "        counts.append(len(list(elements)))\r\n",
        "    compressed_rle = maskutil.frPyObjects(rle, rle.get('size')[0], rle.get('size')[1])\r\n",
        "    compressed_rle['counts'] = str(compressed_rle['counts'], encoding='utf-8')\r\n",
        "    return compressed_rle\r\n",
        "\r\n",
        "@DATASETS.register_module(force=True)\r\n",
        "class VocTiny(CustomDataset):\r\n",
        "    CLASSES = ['aeroplane', \r\n",
        "               'bicycle',            \r\n",
        "               'bird',               \r\n",
        "               'boat',                \r\n",
        "               'bottle',              \r\n",
        "               'bus',              \r\n",
        "               'car',              \r\n",
        "               'cat',              \r\n",
        "               'chair',              \r\n",
        "               'cow',              \r\n",
        "               'diningtable',         \r\n",
        "               'dog',         \r\n",
        "               'horse',         \r\n",
        "               'motorbike',         \r\n",
        "               'person',         \r\n",
        "               'pottedplant',         \r\n",
        "               'sheep',       \r\n",
        "               'sofa',       \r\n",
        "               'train',       \r\n",
        "               'tvmonitor',\r\n",
        "               ]  \r\n",
        "    \r\n",
        "    def load_annotations(self, ann_file):\r\n",
        "        \"\"\"Load annotation from COCO style annotation file.\r\n",
        "        Args:\r\n",
        "            ann_file (str): Path of annotation file.\r\n",
        "        Returns:\r\n",
        "            list[dict]: Annotation info from COCO api.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        self.coco = COCO(ann_file)\r\n",
        "        self.cat_ids = self.coco.getCatIds(catNms=self.CLASSES)\r\n",
        "        self.cat2label = {cat_id: i for i, cat_id in enumerate(self.cat_ids)}\r\n",
        "        self.img_ids = list(self.coco.imgs.keys())\r\n",
        "        self.data_infos = []\r\n",
        "        for i, idx in enumerate(self.img_ids):\r\n",
        "            info = self.coco.loadImgs(idx)[0]\r\n",
        "            info['filename'] = info['file_name']\r\n",
        "            ann = self.get_ann_info(i)\r\n",
        "\r\n",
        "            info['ann'] = ann\r\n",
        "            self.data_infos.append(info)\r\n",
        "        return self.data_infos\r\n",
        "\r\n",
        "    def get_ann_info(self, i):\r\n",
        "        \"\"\"Get COCO annotation by index.\r\n",
        "        Args:\r\n",
        "            idx (int): Index of data.\r\n",
        "        Returns:\r\n",
        "            dict: Annotation info of specified index.\r\n",
        "        \"\"\"\r\n",
        "        idx = list(self.coco.imgs.keys())[i]\r\n",
        "        info = self.coco.loadImgs(idx)[0]\r\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=idx)\r\n",
        "        ann_info = self.coco.loadAnns(ann_ids)\r\n",
        "        return self._parse_ann_info(info, ann_info)\r\n",
        "\r\n",
        "    def get_cat_ids(self, idx):\r\n",
        "        \"\"\"Get COCO category ids by index.\r\n",
        "        Args:\r\n",
        "            idx (int): Index of data.\r\n",
        "        Returns:\r\n",
        "            list[int]: All categories in the image of specified index.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        img_id = self.data_infos[idx]['id']\r\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\r\n",
        "        ann_info = self.coco.loadAnns(ann_ids)\r\n",
        "        return [ann['category_id'] for ann in ann_info]\r\n",
        "\r\n",
        "    def _filter_imgs(self, min_size=32):\r\n",
        "        \"\"\"Filter images too small or without ground truths.\"\"\"\r\n",
        "        valid_inds = []\r\n",
        "        # obtain images that contain annotation\r\n",
        "        ids_with_ann = set(_['image_id'] for _ in self.coco.anns.values())\r\n",
        "        # obtain images that contain annotations of the required categories\r\n",
        "        ids_in_cat = set()\r\n",
        "        for i, class_id in enumerate(self.cat_ids):\r\n",
        "            ids_in_cat |= set(self.coco.cat_img_map[class_id])\r\n",
        "        # merge the image id sets of the two conditions and use the merged set\r\n",
        "        # to filter out images if self.filter_empty_gt=True\r\n",
        "        ids_in_cat &= ids_with_ann\r\n",
        "\r\n",
        "        valid_img_ids = []\r\n",
        "        for i, img_info in enumerate(self.data_infos):\r\n",
        "            img_id = img_info['id']\r\n",
        "            if self.filter_empty_gt and img_id not in ids_in_cat:\r\n",
        "                continue\r\n",
        "            if min(img_info['width'], img_info['height']) >= min_size:\r\n",
        "                valid_inds.append(i)\r\n",
        "                valid_img_ids.append(img_id)\r\n",
        "        self.img_ids = valid_img_ids\r\n",
        "        return valid_inds\r\n",
        "\r\n",
        "    def _parse_ann_info(self, img_info, ann_info):\r\n",
        "        \"\"\"Parse bbox and mask annotation.\r\n",
        "        Args:\r\n",
        "            ann_info (list[dict]): Annotation info of an image.\r\n",
        "            with_mask (bool): Whether to parse mask annotations.\r\n",
        "        Returns:\r\n",
        "            dict: A dict containing the following keys: bboxes, bboxes_ignore,\\\r\n",
        "                labels, masks, seg_map. \"masks\" are raw annotations and not \\\r\n",
        "                decoded into binary masks.\r\n",
        "        \"\"\"\r\n",
        "        gt_bboxes = []\r\n",
        "        gt_labels = []\r\n",
        "        gt_bboxes_ignore = []\r\n",
        "        gt_masks_ann = []\r\n",
        "        for i, ann in enumerate(ann_info):\r\n",
        "            if ann.get('ignore', False):\r\n",
        "                continue\r\n",
        "            x1, y1, w, h = ann['bbox']\r\n",
        "            inter_w = max(0, min(x1 + w, img_info['width']) - max(x1, 0))\r\n",
        "            inter_h = max(0, min(y1 + h, img_info['height']) - max(y1, 0))\r\n",
        "            if inter_w * inter_h == 0:\r\n",
        "                continue\r\n",
        "            if ann['area'] <= 0 or w < 1 or h < 1:\r\n",
        "                continue\r\n",
        "            if ann['category_id'] not in self.cat_ids:\r\n",
        "                continue\r\n",
        "            bbox = [x1, y1, x1 + w, y1 + h]\r\n",
        "            if ann.get('iscrowd', False):\r\n",
        "                gt_bboxes_ignore.append(bbox)\r\n",
        "            else:\r\n",
        "                gt_bboxes.append(bbox)\r\n",
        "                gt_labels.append(self.cat2label[ann['category_id']])\r\n",
        "                gt_masks_ann.append(ann.get('segmentation', None))\r\n",
        "\r\n",
        "        if gt_bboxes:\r\n",
        "            gt_bboxes = np.array(gt_bboxes, dtype=np.float32)\r\n",
        "            gt_labels = np.array(gt_labels, dtype=np.int64)\r\n",
        "        else:\r\n",
        "            gt_bboxes = np.zeros((0, 4), dtype=np.float32)\r\n",
        "            gt_labels = np.array([], dtype=np.int64)\r\n",
        "\r\n",
        "        if gt_bboxes_ignore:\r\n",
        "            gt_bboxes_ignore = np.array(gt_bboxes_ignore, dtype=np.float32)\r\n",
        "        else:\r\n",
        "            gt_bboxes_ignore = np.zeros((0, 4), dtype=np.float32)\r\n",
        "\r\n",
        "        seg_map = img_info['filename'].replace('jpg', 'png')\r\n",
        "\r\n",
        "        ann = dict(\r\n",
        "            bboxes=gt_bboxes,\r\n",
        "            labels=gt_labels,\r\n",
        "            bboxes_ignore=gt_bboxes_ignore,\r\n",
        "            masks=gt_masks_ann,\r\n",
        "            seg_map=seg_map)\r\n",
        "\r\n",
        "        return ann\r\n",
        "\r\n",
        "    def xyxy2xywh(self, bbox):\r\n",
        "        \"\"\"Convert ``xyxy`` style bounding boxes to ``xywh`` style for COCO\r\n",
        "        evaluation.\r\n",
        "        Args:\r\n",
        "            bbox (numpy.ndarray): The bounding boxes, shape (4, ), in\r\n",
        "                ``xyxy`` order.\r\n",
        "        Returns:\r\n",
        "            list[float]: The converted bounding boxes, in ``xywh`` order.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        _bbox = bbox.tolist()\r\n",
        "        return [\r\n",
        "            _bbox[0],\r\n",
        "            _bbox[1],\r\n",
        "            _bbox[2] - _bbox[0],\r\n",
        "            _bbox[3] - _bbox[1],\r\n",
        "        ]\r\n",
        "\r\n",
        "    def _proposal2json(self, results):\r\n",
        "        \"\"\"Convert proposal results to COCO json style.\"\"\"\r\n",
        "        json_results = []\r\n",
        "        for idx in range(len(self)):\r\n",
        "            img_id = self.img_ids[idx]\r\n",
        "            bboxes = results[idx]\r\n",
        "            for i in range(bboxes.shape[0]):\r\n",
        "                data = dict()\r\n",
        "                data['image_id'] = img_id\r\n",
        "                data['bbox'] = self.xyxy2xywh(bboxes[i])\r\n",
        "                data['score'] = float(bboxes[i][4])\r\n",
        "                data['category_id'] = 1\r\n",
        "                json_results.append(data)\r\n",
        "        return json_results\r\n",
        "\r\n",
        "    def _det2json(self, results):\r\n",
        "        \"\"\"Convert detection results to COCO json style.\"\"\"\r\n",
        "        json_results = []\r\n",
        "        for idx in range(len(self)):\r\n",
        "            img_id = self.img_ids[idx]\r\n",
        "            result = results[idx]\r\n",
        "            for label in range(len(result)):\r\n",
        "                bboxes = result[label]\r\n",
        "                for i in range(bboxes.shape[0]):\r\n",
        "                    data = dict()\r\n",
        "                    data['image_id'] = img_id\r\n",
        "                    data['bbox'] = self.xyxy2xywh(bboxes[i])\r\n",
        "                    data['score'] = float(bboxes[i][4])\r\n",
        "                    data['category_id'] = self.cat_ids[label]\r\n",
        "                    json_results.append(data)\r\n",
        "        return json_results\r\n",
        "\r\n",
        "    def _segm2json(self, results):\r\n",
        "        \"\"\"Convert instance segmentation results to COCO json style.\"\"\"\r\n",
        "        bbox_json_results = []\r\n",
        "        segm_json_results = []\r\n",
        "        for idx in range(len(results)):\r\n",
        "            \r\n",
        "            img_id = self.img_ids[idx]\r\n",
        "            det, seg = results[idx]\r\n",
        "            for label in range(len(det)):\r\n",
        "                # bbox results\r\n",
        "                bboxes = det[label]\r\n",
        "                for i in range(bboxes.shape[0]):\r\n",
        "                    data = dict()\r\n",
        "                    data['image_id'] = img_id\r\n",
        "                    data['bbox'] = self.xyxy2xywh(bboxes[i])\r\n",
        "                    data['score'] = float(bboxes[i][4])\r\n",
        "                    data['category_id'] = self.cat_ids[label]\r\n",
        "                    bbox_json_results.append(data)\r\n",
        "\r\n",
        "                # segm results\r\n",
        "                # some detectors use different scores for bbox and mask\r\n",
        "                if isinstance(seg, tuple):\r\n",
        "                    segms = seg[0][label]\r\n",
        "                    mask_score = seg[1][label]\r\n",
        "                else:\r\n",
        "                    segms = seg[label]\r\n",
        "                    mask_score = [bbox[4] for bbox in bboxes]\r\n",
        "                for i in range(bboxes.shape[0]):\r\n",
        "                    data = dict()\r\n",
        "                    data['image_id'] = img_id\r\n",
        "                    #data['bbox'] = self.xyxy2xywh(bboxes[i])\r\n",
        "                    data['score'] = float(mask_score[i])\r\n",
        "                    data['category_id'] = self.cat_ids[label]\r\n",
        "                    rle = binary_mask_to_rle(segms[i])\r\n",
        "                    data['segmentation'] = rle\r\n",
        "                    segm_json_results.append(data)\r\n",
        "        return bbox_json_results, segm_json_results\r\n",
        "\r\n",
        "    def results2json(self, results, outfile_prefix):\r\n",
        "        \"\"\"Dump the detection results to a COCO style json file.\r\n",
        "        There are 3 types of results: proposals, bbox predictions, mask\r\n",
        "        predictions, and they have different data types. This method will\r\n",
        "        automatically recognize the type, and dump them to json files.\r\n",
        "        Args:\r\n",
        "            results (list[list | tuple | ndarray]): Testing results of the\r\n",
        "                dataset.\r\n",
        "            outfile_prefix (str): The filename prefix of the json files. If the\r\n",
        "                prefix is \"somepath/xxx\", the json files will be named\r\n",
        "                \"somepath/xxx.bbox.json\", \"somepath/xxx.segm.json\",\r\n",
        "                \"somepath/xxx.proposal.json\".\r\n",
        "        Returns:\r\n",
        "            dict[str: str]: Possible keys are \"bbox\", \"segm\", \"proposal\", and \\\r\n",
        "                values are corresponding filenames.\r\n",
        "        \"\"\"\r\n",
        "        result_files = dict()\r\n",
        "        if isinstance(results[0], list):\r\n",
        "            json_results = self._det2json(results)\r\n",
        "            result_files['bbox'] = f'{outfile_prefix}.bbox.json'\r\n",
        "            result_files['proposal'] = f'{outfile_prefix}.bbox.json'\r\n",
        "            mmcv.dump(json_results, result_files['bbox'])\r\n",
        "        elif isinstance(results[0], tuple):\r\n",
        "            json_results = self._segm2json(results)\r\n",
        "            result_files['bbox'] = f'{outfile_prefix}.bbox.json'\r\n",
        "            result_files['proposal'] = f'{outfile_prefix}.bbox.json'\r\n",
        "            result_files['segm'] = f'{outfile_prefix}.segm.json'\r\n",
        "            mmcv.dump(json_results[0], result_files['bbox'])\r\n",
        "            mmcv.dump(json_results[1], result_files['segm'])\r\n",
        "        elif isinstance(results[0], np.ndarray):\r\n",
        "            json_results = self._proposal2json(results)\r\n",
        "            result_files['proposal'] = f'{outfile_prefix}.proposal.json'\r\n",
        "            mmcv.dump(json_results, result_files['proposal'])\r\n",
        "        else:\r\n",
        "            raise TypeError('invalid type of results')\r\n",
        "        return result_files\r\n",
        "\r\n",
        "    def fast_eval_recall(self, results, proposal_nums, iou_thrs, logger=None):\r\n",
        "        gt_bboxes = []\r\n",
        "        for i in range(len(self.img_ids)):\r\n",
        "            ann_ids = self.coco.getAnnIds(imgIds=self.img_ids[i])\r\n",
        "            ann_info = self.coco.loadAnns(ann_ids)\r\n",
        "            if len(ann_info) == 0:\r\n",
        "                gt_bboxes.append(np.zeros((0, 4)))\r\n",
        "                continue\r\n",
        "            bboxes = []\r\n",
        "            for ann in ann_info:\r\n",
        "                if ann.get('ignore', False) or ann['iscrowd']:\r\n",
        "                    continue\r\n",
        "                x1, y1, w, h = ann['bbox']\r\n",
        "                bboxes.append([x1, y1, x1 + w, y1 + h])\r\n",
        "            bboxes = np.array(bboxes, dtype=np.float32)\r\n",
        "            if bboxes.shape[0] == 0:\r\n",
        "                bboxes = np.zeros((0, 4))\r\n",
        "            gt_bboxes.append(bboxes)\r\n",
        "\r\n",
        "        recalls = eval_recalls(\r\n",
        "            gt_bboxes, results, proposal_nums, iou_thrs, logger=logger)\r\n",
        "        ar = recalls.mean(axis=1)\r\n",
        "        return ar\r\n",
        "\r\n",
        "    def format_results(self, results, jsonfile_prefix=None, **kwargs):\r\n",
        "        \"\"\"Format the results to json (standard format for COCO evaluation).\r\n",
        "        Args:\r\n",
        "            results (list[tuple | numpy.ndarray]): Testing results of the\r\n",
        "                dataset.\r\n",
        "            jsonfile_prefix (str | None): The prefix of json files. It includes\r\n",
        "                the file path and the prefix of filename, e.g., \"a/b/prefix\".\r\n",
        "                If not specified, a temp file will be created. Default: None.\r\n",
        "        Returns:\r\n",
        "            tuple: (result_files, tmp_dir), result_files is a dict containing \\\r\n",
        "                the json filepaths, tmp_dir is the temporal directory created \\\r\n",
        "                for saving json files when jsonfile_prefix is not specified.\r\n",
        "        \"\"\"\r\n",
        "        assert isinstance(results, list), 'results must be a list'\r\n",
        "        assert len(results) == len(self), (\r\n",
        "            'The length of results is not equal to the dataset len: {} != {}'.\r\n",
        "            format(len(results), len(self)))\r\n",
        "\r\n",
        "        if jsonfile_prefix is None:\r\n",
        "            tmp_dir = tempfile.TemporaryDirectory()\r\n",
        "            jsonfile_prefix = osp.join(tmp_dir.name, 'results')\r\n",
        "        else:\r\n",
        "            tmp_dir = None\r\n",
        "        result_files = self.results2json(results, jsonfile_prefix)\r\n",
        "        return result_files, tmp_dir\r\n",
        "\r\n",
        "    def evaluate(self,\r\n",
        "                 results,\r\n",
        "                 metric='bbox',\r\n",
        "                 logger=None,\r\n",
        "                 jsonfile_prefix=None,\r\n",
        "                 classwise=False,\r\n",
        "                 proposal_nums=(100, 300, 1000),\r\n",
        "                 iou_thrs=None,\r\n",
        "                 metric_items=None):\r\n",
        "        \"\"\"Evaluation in COCO protocol.\r\n",
        "        Args:\r\n",
        "            results (list[list | tuple]): Testing results of the dataset.\r\n",
        "            metric (str | list[str]): Metrics to be evaluated. Options are\r\n",
        "                'bbox', 'segm', 'proposal', 'proposal_fast'.\r\n",
        "            logger (logging.Logger | str | None): Logger used for printing\r\n",
        "                related information during evaluation. Default: None.\r\n",
        "            jsonfile_prefix (str | None): The prefix of json files. It includes\r\n",
        "                the file path and the prefix of filename, e.g., \"a/b/prefix\".\r\n",
        "                If not specified, a temp file will be created. Default: None.\r\n",
        "            classwise (bool): Whether to evaluating the AP for each class.\r\n",
        "            proposal_nums (Sequence[int]): Proposal number used for evaluating\r\n",
        "                recalls, such as recall@100, recall@1000.\r\n",
        "                Default: (100, 300, 1000).\r\n",
        "            iou_thrs (Sequence[float], optional): IoU threshold used for\r\n",
        "                evaluating recalls/mAPs. If set to a list, the average of all\r\n",
        "                IoUs will also be computed. If not specified, [0.50, 0.55,\r\n",
        "                0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used.\r\n",
        "                Default: None.\r\n",
        "            metric_items (list[str] | str, optional): Metric items that will\r\n",
        "                be returned. If not specified, ``['AR@100', 'AR@300',\r\n",
        "                'AR@1000', 'AR_s@1000', 'AR_m@1000', 'AR_l@1000' ]`` will be\r\n",
        "                used when ``metric=='proposal'``, ``['mAP', 'mAP_50', 'mAP_75',\r\n",
        "                'mAP_s', 'mAP_m', 'mAP_l']`` will be used when\r\n",
        "                ``metric=='bbox' or metric=='segm'``.\r\n",
        "        Returns:\r\n",
        "            dict[str, float]: COCO style evaluation metric.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        metrics = metric if isinstance(metric, list) else [metric]\r\n",
        "        allowed_metrics = ['bbox', 'segm', 'proposal', 'proposal_fast']\r\n",
        "        for metric in metrics:\r\n",
        "            if metric not in allowed_metrics:\r\n",
        "                raise KeyError(f'metric {metric} is not supported')\r\n",
        "        if iou_thrs is None:\r\n",
        "            iou_thrs = np.linspace(\r\n",
        "                .5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)\r\n",
        "        if metric_items is not None:\r\n",
        "            if not isinstance(metric_items, list):\r\n",
        "                metric_items = [metric_items]\r\n",
        "\r\n",
        "        result_files, tmp_dir = self.format_results(results, jsonfile_prefix)\r\n",
        "\r\n",
        "        eval_results = OrderedDict()\r\n",
        "        cocoGt = self.coco\r\n",
        "        for metric in metrics:\r\n",
        "            msg = f'Evaluating {metric}...'\r\n",
        "            if logger is None:\r\n",
        "                msg = '\\n' + msg\r\n",
        "            print_log(msg, logger=logger)\r\n",
        "\r\n",
        "            if metric == 'proposal_fast':\r\n",
        "                ar = self.fast_eval_recall(\r\n",
        "                    results, proposal_nums, iou_thrs, logger='silent')\r\n",
        "                log_msg = []\r\n",
        "                for i, num in enumerate(proposal_nums):\r\n",
        "                    eval_results[f'AR@{num}'] = ar[i]\r\n",
        "                    log_msg.append(f'\\nAR@{num}\\t{ar[i]:.4f}')\r\n",
        "                log_msg = ''.join(log_msg)\r\n",
        "                print_log(log_msg, logger=logger)\r\n",
        "                continue\r\n",
        "\r\n",
        "            if metric not in result_files:\r\n",
        "                raise KeyError(f'{metric} is not in results')\r\n",
        "            try:\r\n",
        "                cocoDt = cocoGt.loadRes(result_files[metric])\r\n",
        "            except IndexError:\r\n",
        "                print_log(\r\n",
        "                    'The testing results of the whole dataset is empty.',\r\n",
        "                    logger=logger,\r\n",
        "                    level=logging.ERROR)\r\n",
        "                break\r\n",
        "\r\n",
        "            iou_type = 'bbox' if metric == 'proposal' else metric\r\n",
        "            cocoEval = COCOeval(cocoGt, cocoDt, iou_type)\r\n",
        "            cocoEval.params.catIds = self.cat_ids\r\n",
        "            cocoEval.params.imgIds = self.img_ids\r\n",
        "            cocoEval.params.maxDets = list(proposal_nums)\r\n",
        "            cocoEval.params.iouThrs = iou_thrs\r\n",
        "            # mapping of cocoEval.stats\r\n",
        "            coco_metric_names = {\r\n",
        "                'mAP': 0,\r\n",
        "                'mAP_50': 1,\r\n",
        "                'mAP_75': 2,\r\n",
        "                'mAP_s': 3,\r\n",
        "                'mAP_m': 4,\r\n",
        "                'mAP_l': 5,\r\n",
        "                'AR@100': 6,\r\n",
        "                'AR@300': 7,\r\n",
        "                'AR@1000': 8,\r\n",
        "                'AR_s@1000': 9,\r\n",
        "                'AR_m@1000': 10,\r\n",
        "                'AR_l@1000': 11\r\n",
        "            }\r\n",
        "            if metric_items is not None:\r\n",
        "                for metric_item in metric_items:\r\n",
        "                    if metric_item not in coco_metric_names:\r\n",
        "                        raise KeyError(\r\n",
        "                            f'metric item {metric_item} is not supported')\r\n",
        "\r\n",
        "            if metric == 'proposal':\r\n",
        "                cocoEval.params.useCats = 0\r\n",
        "                cocoEval.evaluate()\r\n",
        "                cocoEval.accumulate()\r\n",
        "                cocoEval.summarize()\r\n",
        "                if metric_items is None:\r\n",
        "                    metric_items = [\r\n",
        "                        'AR@100', 'AR@300', 'AR@1000', 'AR_s@1000',\r\n",
        "                        'AR_m@1000', 'AR_l@1000'\r\n",
        "                    ]\r\n",
        "\r\n",
        "                for item in metric_items:\r\n",
        "                    val = float(\r\n",
        "                        f'{cocoEval.stats[coco_metric_names[item]]:.3f}')\r\n",
        "                    eval_results[item] = val\r\n",
        "            else:\r\n",
        "                cocoEval.evaluate()\r\n",
        "                cocoEval.accumulate()\r\n",
        "                cocoEval.summarize()\r\n",
        "                if classwise:  # Compute per-category AP\r\n",
        "                    # Compute per-category AP\r\n",
        "                    # from https://github.com/facebookresearch/detectron2/\r\n",
        "                    precisions = cocoEval.eval['precision']\r\n",
        "                    # precision: (iou, recall, cls, area range, max dets)\r\n",
        "                    assert len(self.cat_ids) == precisions.shape[2]\r\n",
        "\r\n",
        "                    results_per_category = []\r\n",
        "                    for idx, catId in enumerate(self.cat_ids):\r\n",
        "                        # area range index 0: all area ranges\r\n",
        "                        # max dets index -1: typically 100 per image\r\n",
        "                        nm = self.coco.loadCats(catId)[0]\r\n",
        "                        precision = precisions[:, :, idx, 0, -1]\r\n",
        "                        precision = precision[precision > -1]\r\n",
        "                        if precision.size:\r\n",
        "                            ap = np.mean(precision)\r\n",
        "                        else:\r\n",
        "                            ap = float('nan')\r\n",
        "                        results_per_category.append(\r\n",
        "                            (f'{nm[\"name\"]}', f'{float(ap):0.3f}'))\r\n",
        "\r\n",
        "                    num_columns = min(6, len(results_per_category) * 2)\r\n",
        "                    results_flatten = list(\r\n",
        "                        itertools.chain(*results_per_category))\r\n",
        "                    headers = ['category', 'AP'] * (num_columns // 2)\r\n",
        "                    results_2d = itertools.zip_longest(*[\r\n",
        "                        results_flatten[i::num_columns]\r\n",
        "                        for i in range(num_columns)\r\n",
        "                    ])\r\n",
        "                    table_data = [headers]\r\n",
        "                    table_data += [result for result in results_2d]\r\n",
        "                    table = AsciiTable(table_data)\r\n",
        "                    print_log('\\n' + table.table, logger=logger)\r\n",
        "\r\n",
        "                if metric_items is None:\r\n",
        "                    metric_items = [\r\n",
        "                        'mAP', 'mAP_50', 'mAP_75', 'mAP_s', 'mAP_m', 'mAP_l'\r\n",
        "                    ]\r\n",
        "\r\n",
        "                for metric_item in metric_items:\r\n",
        "                    key = f'{metric}_{metric_item}'\r\n",
        "                    val = float(\r\n",
        "                        f'{cocoEval.stats[coco_metric_names[metric_item]]:.3f}'\r\n",
        "                    )\r\n",
        "                    eval_results[key] = val\r\n",
        "                ap = cocoEval.stats[:6]\r\n",
        "                eval_results[f'{metric}_mAP_copypaste'] = (\r\n",
        "                    f'{ap[0]:.3f} {ap[1]:.3f} {ap[2]:.3f} {ap[3]:.3f} '\r\n",
        "                    f'{ap[4]:.3f} {ap[5]:.3f}')\r\n",
        "        if tmp_dir is not None:\r\n",
        "            tmp_dir.cleanup()\r\n",
        "        return eval_results\r\n",
        "\r\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AmRaRjO0VJMk",
        "outputId": "d2a96d5a-b29e-4cfc-df2b-843554ecfd6d"
      },
      "source": [
        "# Build dataset\r\n",
        "datasets = [build_dataset(cfg.data.train)]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Build the detector\r\n",
        "model = build_detector(\r\n",
        "    cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\r\n",
        "# Add an attribute for visualization convenience\r\n",
        "model.CLASSES = datasets[0].CLASSES\r\n",
        "\r\n",
        "# Create work_dir\r\n",
        "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\r\n",
        "train_detector(model, datasets, cfg, distributed=False, validate=False)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.35s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-11 15:04:12,185 - mmdet - INFO - load model from: torchvision://resnet50\n",
            "2020-12-11 15:04:12,411 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
            "\n",
            "unexpected key in source state_dict: fc.weight, fc.bias\n",
            "\n",
            "2020-12-11 15:04:12,600 - mmdet - INFO - Start running, host: root@1aed3086d2c8, work_dir: /content/gdrive/MyDrive/T0828/HW3/log\n",
            "2020-12-11 15:04:12,600 - mmdet - INFO - workflow: [('train', 1)], max: 24 epochs\n",
            "2020-12-11 15:05:16,416 - mmdet - INFO - Epoch [1][50/338]\tlr: 1.978e-03, eta: 2:50:47, time: 1.271, data_time: 0.092, memory: 7103, loss_rpn_cls: 0.5140, loss_rpn_bbox: 0.0452, loss_cls: 0.7765, acc: 89.9277, loss_bbox: 0.0359, loss_mask: 0.8083, loss: 2.1800\n",
            "2020-12-11 15:06:18,209 - mmdet - INFO - Epoch [1][100/338]\tlr: 3.976e-03, eta: 2:47:24, time: 1.236, data_time: 0.037, memory: 7103, loss_rpn_cls: 0.0984, loss_rpn_bbox: 0.0418, loss_cls: 0.2651, acc: 96.5361, loss_bbox: 0.1343, loss_mask: 0.6875, loss: 1.2271\n",
            "2020-12-11 15:07:20,736 - mmdet - INFO - Epoch [1][150/338]\tlr: 5.974e-03, eta: 2:46:12, time: 1.250, data_time: 0.035, memory: 7103, loss_rpn_cls: 0.0801, loss_rpn_bbox: 0.0409, loss_cls: 0.2915, acc: 95.9580, loss_bbox: 0.1618, loss_mask: 0.6684, loss: 1.2427\n",
            "Process Process-100:\n",
            "Process Process-97:\n",
            "Process Process-98:\n",
            "Process Process-99:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
            "    util._exit_function()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
            "    _run_finalizers()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
            "    thread.join()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1056, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
            "    util._exit_function()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
            "    _run_finalizers()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
            "    util._exit_function()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
            "    util._exit_function()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
            "    _run_finalizers()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
            "    _run_finalizers()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
            "    thread.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
            "    thread.join()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1056, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1056, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
            "    thread.join()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1056, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6b7eb16080>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 124, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 47, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n",
            "    fd_event_list = self._poll.poll(timeout)\n",
            "KeyboardInterrupt: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-250f2642843d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Create work_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmmcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir_or_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/mmdetection/mmdet/apis/train.py\u001b[0m in \u001b[0;36mtrain_detector\u001b[0;34m(model, dataset, cfg, distributed, validate, timestamp, meta)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_loaders, workflow, max_epochs, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                     \u001b[0mepoch_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# wait for some hooks like loggers to finish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before_train_iter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_train_iter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mmcv/runner/epoch_based_runner.py\u001b[0m in \u001b[0;36mrun_iter\u001b[0;34m(self, data_batch, train_mode, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             outputs = self.model.train_step(data_batch, self.optimizer,\n\u001b[0;32m---> 30\u001b[0;31m                                             **kwargs)\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mmcv/parallel/data_parallel.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmdetection/mmdet/models/detectors/base.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data, optimizer)\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0maveraging\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mmcv/runner/fp16_utils.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m                                 'method of nn.Module')\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fp16_enabled'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16_enabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mold_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;31m# get the arg spec of the decorated method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0margs_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmdetection/mmdet/models/detectors/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, img_metas, return_loss, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_metas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_metas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmdetection/mmdet/models/detectors/two_stage.py\u001b[0m in \u001b[0;36mforward_train\u001b[0;34m(self, img, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore, gt_masks, proposals, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mgt_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mgt_bboxes_ignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgt_bboxes_ignore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 proposal_cfg=proposal_cfg)\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpn_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmdetection/mmdet/models/dense_heads/base_dense_head.py\u001b[0m in \u001b[0;36mforward_train\u001b[0;34m(self, x, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore, proposal_cfg, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mloss_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouts\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgt_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_metas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_bboxes_ignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgt_bboxes_ignore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproposal_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmdetection/mmdet/models/dense_heads/rpn_head.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, cls_scores, bbox_preds, gt_bboxes, img_metas, gt_bboxes_ignore)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mimg_metas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             gt_bboxes_ignore=gt_bboxes_ignore)\n\u001b[0m\u001b[1;32m     75\u001b[0m         return dict(\n\u001b[1;32m     76\u001b[0m             loss_rpn_cls=losses['loss_cls'], loss_rpn_bbox=losses['loss_bbox'])\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mmcv/runner/fp16_utils.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                                 'method of nn.Module')\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fp16_enabled'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16_enabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mold_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;31m# get the arg spec of the decorated method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0margs_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmdetection/mmdet/models/dense_heads/anchor_head.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         anchor_list, valid_flag_list = self.get_anchors(\n\u001b[0;32m--> 452\u001b[0;31m             featmap_sizes, img_metas, device=device)\n\u001b[0m\u001b[1;32m    453\u001b[0m         \u001b[0mlabel_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_out_channels\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_sigmoid_cls\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         cls_reg_targets = self.get_targets(\n",
            "\u001b[0;32m/content/mmdetection/mmdet/models/dense_heads/anchor_head.py\u001b[0m in \u001b[0;36mget_anchors\u001b[0;34m(self, featmap_sizes, img_metas, device)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;31m# anchors for one time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         multi_level_anchors = self.anchor_generator.grid_anchors(\n\u001b[0;32m--> 160\u001b[0;31m             featmap_sizes, device)\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0manchor_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmulti_level_anchors\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmdetection/mmdet/core/anchor/anchor_generator.py\u001b[0m in \u001b[0;36mgrid_anchors\u001b[0;34m(self, featmap_sizes, device)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_levels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             anchors = self.single_level_grid_anchors(\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_anchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mfeatmap_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFstNDwnIRdI",
        "outputId": "211ce082-f713-4ce0-87db-02e6d8a0761f"
      },
      "source": [
        "import os\r\n",
        "test_dataset = build_dataset(cfg.data.test)\r\n",
        "info = test_dataset.load_annotations(os.path.join(cfg.data_root,cfg.data.test.ann_file))\r\n",
        "## test ##\r\n",
        "#for i in range(len(info)):\r\n",
        "    #print(info[i]['filename'], info[i]['id'], test_dataset.img_ids[i])\r\n",
        "model.cfg = cfg\r\n",
        "results = []\r\n",
        "for i in range(len(info)):\r\n",
        "    img = mmcv.imread(os.path.join(cfg.data_root,cfg.data.test.img_prefix, info[i]['filename']))\r\n",
        "    result = inference_detector(model, img)\r\n",
        "    results.append(result)\r\n",
        "test_dataset.results2json(results, '/content/gdrive/MyDrive/T0828/HW3/result')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bbox': '/content/gdrive/MyDrive/T0828/HW3/result.bbox.json',\n",
              " 'proposal': '/content/gdrive/MyDrive/T0828/HW3/result.bbox.json',\n",
              " 'segm': '/content/gdrive/MyDrive/T0828/HW3/result.segm.json'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsoA4KaBI7oQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}